{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Lane Finding Project\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found object points and image points in: ./camera_cal\\calibration10.jpg\n",
      "Write ChessboardCorners to: ./camera_cal/found/calibration10.jpg\n",
      "Found object points and image points in: ./camera_cal\\calibration11.jpg\n",
      "Write ChessboardCorners to: ./camera_cal/found/calibration11.jpg\n",
      "Found object points and image points in: ./camera_cal\\calibration12.jpg\n",
      "Write ChessboardCorners to: ./camera_cal/found/calibration12.jpg\n",
      "Found object points and image points in: ./camera_cal\\calibration13.jpg\n",
      "Write ChessboardCorners to: ./camera_cal/found/calibration13.jpg\n",
      "Found object points and image points in: ./camera_cal\\calibration14.jpg\n",
      "Write ChessboardCorners to: ./camera_cal/found/calibration14.jpg\n",
      "Found object points and image points in: ./camera_cal\\calibration15.jpg\n",
      "Write ChessboardCorners to: ./camera_cal/found/calibration15.jpg\n",
      "Found object points and image points in: ./camera_cal\\calibration16.jpg\n",
      "Write ChessboardCorners to: ./camera_cal/found/calibration16.jpg\n",
      "Found object points and image points in: ./camera_cal\\calibration17.jpg\n",
      "Write ChessboardCorners to: ./camera_cal/found/calibration17.jpg\n",
      "Found object points and image points in: ./camera_cal\\calibration18.jpg\n",
      "Write ChessboardCorners to: ./camera_cal/found/calibration18.jpg\n",
      "Found object points and image points in: ./camera_cal\\calibration19.jpg\n",
      "Write ChessboardCorners to: ./camera_cal/found/calibration19.jpg\n",
      "Found object points and image points in: ./camera_cal\\calibration2.jpg\n",
      "Write ChessboardCorners to: ./camera_cal/found/calibration2.jpg\n",
      "Found object points and image points in: ./camera_cal\\calibration20.jpg\n",
      "Write ChessboardCorners to: ./camera_cal/found/calibration20.jpg\n",
      "Found object points and image points in: ./camera_cal\\calibration3.jpg\n",
      "Write ChessboardCorners to: ./camera_cal/found/calibration3.jpg\n",
      "Found object points and image points in: ./camera_cal\\calibration6.jpg\n",
      "Write ChessboardCorners to: ./camera_cal/found/calibration6.jpg\n",
      "Found object points and image points in: ./camera_cal\\calibration7.jpg\n",
      "Write ChessboardCorners to: ./camera_cal/found/calibration7.jpg\n",
      "Found object points and image points in: ./camera_cal\\calibration8.jpg\n",
      "Write ChessboardCorners to: ./camera_cal/found/calibration8.jpg\n",
      "Found object points and image points in: ./camera_cal\\calibration9.jpg\n",
      "Write ChessboardCorners to: ./camera_cal/found/calibration9.jpg\n",
      "(17, 54, 3)\n",
      "(17, 54, 1, 2)\n",
      "[[  1.15777818e+03   0.00000000e+00   6.67113857e+02]\n",
      " [  0.00000000e+00   1.15282217e+03   3.86124583e+02]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00]]\n",
      "[[-0.24688507 -0.02373156 -0.00109831  0.00035107 -0.00259866]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((6*9,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)\n",
    "\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d points in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "# Make a list of calibration images\n",
    "images = glob.glob('./camera_cal/calibration*.jpg')\n",
    "\n",
    "# Step through the list and search for chessboard corners\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (9,6),None)\n",
    "\n",
    "    # If found, add object points, image points\n",
    "    if ret == True:\n",
    "        print('Found object points and image points in: ' + fname)\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners)\n",
    "        \n",
    "        # Draw and display the corners\n",
    "        img = cv2.drawChessboardCorners(img, (9,6), corners, ret)\n",
    "        output_fname = './camera_cal/found/' + fname.split('\\\\')[-1]\n",
    "        print('Write ChessboardCorners to: ' + output_fname)\n",
    "        #cv2.imwrite(output_fname, img)\n",
    "        #plt.imshow(img)\n",
    "        #cv2.waitKey(500)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "dist_img = cv2.imread('./camera_cal/calibration20.jpg')\n",
    "img_size = (dist_img.shape[1], dist_img.shape[0])\n",
    "print(np.array(objpoints).shape)\n",
    "print(np.array(imgpoints).shape)\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_size, None, None)\n",
    "print(mtx)\n",
    "print(dist)\n",
    "\n",
    "dist_pickle = {}\n",
    "dist_pickle['mtx'] = mtx\n",
    "dist_pickle['dist'] = dist\n",
    "pickle.dump( dist_pickle, open( \"./calibration_pickle.p\", \"wb\" ))\n",
    "\n",
    "undist_img = cv2.undistort(dist_img, mtx, dist, None, mtx)\n",
    "\n",
    "#cv2.imshow('undist', undist_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt5\n",
    "\n",
    "def undistort_image(dist_img):\n",
    "    dist_pickel = pickle.load(open(\"./calibration_pickle.p\", \"rb\"))\n",
    "    mtx = dist_pickel[\"mtx\"]\n",
    "    dist = dist_pickel[\"dist\"]\n",
    "        \n",
    "    undist_img = cv2.undistort(dist_img, mtx, dist, None, mtx)\n",
    "    \n",
    "    return undist_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname = \"./test_images/straight_lines2.jpg\"\n",
    "image = cv2.imread(fname)\n",
    "undist_img = undistort_image(image)\n",
    "\n",
    "output_fname = \"./output_images/\" + fname.split('/')[-1]\n",
    "cv2.imwrite(output_fname, undist_img)\n",
    "cv2.imshow('undist', undist_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "src = np.float32([[190,720], [1120,720], [690, 450], [590,450]])\n",
    "#src = np.float32([[200,720],[1100,720],[680,450],[600,450]])\n",
    "dst = np.float32([[320,720], [960,720], [960,0], [320, 0]])\n",
    "M = cv2.getPerspectiveTransform(src, dst)\n",
    "transformed = cv2.warpPerspective(undist_img, M, (undist_img.shape[1], undist_img.shape[0]))\n",
    "\n",
    "cv2.imshow('transformed', transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mag_thresh(img, sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # 2) Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # 3) Calculate the magnitude\n",
    "    gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    # 4) Scale to 8-bit (0 - 255) and convert to type = np.uint8\n",
    "    scaled_sobel = np.uint8(255.* gradmag / np.max(gradmag))\n",
    "    # 5) Create a binary mask where mag thresholds are met\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    #print(binary_output)\n",
    "    binary_output[(scaled_sobel > mag_thresh[0]) & (scaled_sobel < mag_thresh[1])] = 255.\n",
    "    #print(binary_output)\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    #cv2.imshow('scaled_sobel', scaled_sobel)\n",
    "    return binary_output\n",
    "\n",
    "binary_output = mag_thresh(undist_img, sobel_kernel=3, mag_thresh=(120,250))\n",
    "\n",
    "cv2.imshow('binary', binary_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Draw a mask on top of an image\n",
    "def add_binary_mask(img, m):\n",
    "    m2 = np.zeros_like(img)\n",
    "    m2[:, :, 0] = m*255\n",
    "    m2[:, :, 1] = m\n",
    "    m2[:, :, 2] = m\n",
    "    img = np.where(m2, m2, img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def corners_unwarp(img, nx, ny, mtx, dist):\n",
    "    undist_img = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    gray = cv2.cvtColor(undist_img, cv2.COLOR_BGR2GRAY)\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "    if ret:\n",
    "            gray = cv2.drawChessboardCorners(gray, (nx, ny), corners, ret)\n",
    "            offset = 100\n",
    "            img_size = (undist_img.shape[1], undist_img.shape[0])\n",
    "            src = np.float32([corners[0], corners[nx-1], corners[-1], corners[-nx]])\n",
    "            dst = np.float32([[offset, offset], [img_size[0]-offset, offset], \n",
    "                                     [img_size[0]-offset, img_size[1]-offset], \n",
    "                                     [offset, img_size[1]-offset]])\n",
    "            M = cv2.getPerspectiveTransform(src, dst)\n",
    "            warped = cv2.warpPerspective(undist_img, M, gray.shape[::-1])\n",
    "    return warped, M\n",
    "\n",
    "def mag_thresh(img, sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # 2) Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # 3) Calculate the magnitude\n",
    "    mag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    # 4) Scale to 8-bit (0 - 255) and convert to type = np.uint8\n",
    "    scaled_sobel = np.uint8(255.*mag/np.max(mag))\n",
    "    # 5) Create a binary mask where mag thresholds are met\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    binary_output[(scaled_sobel > mag_thresh[0]) & (scaled_sobel < mag_thresh[1])] = 1\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    return binary_output\n",
    "\n",
    "def dir_threshold(img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # 2) Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # 3) Take the absolute value of the x and y gradients\n",
    "    abs_sobelx = np.absolute(sobelx)\n",
    "    abs_sobely = np.absolute(sobely)\n",
    "    # 4) Use np.arctan2(abs_sobely, abs_sobelx) to calculate the direction of the gradient \n",
    "    dir = np.arctan2(abs_sobely, abs_sobelx)\n",
    "    # 5) Create a binary mask where direction thresholds are met\n",
    "    binary_output = np.zeros_like(dir)\n",
    "    binary_output[(dir > thresh[0]) & (dir < thresh[1])] =1\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    return binary_output\n",
    "\n",
    "def hls_select(img, thresh=(0, 255)):\n",
    "    # 1) Convert to HLS color space\n",
    "    hls_img = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    # 2) Apply a threshold to the S channel\n",
    "    s_channel = hls_img[:,:,2]\n",
    "    binary_output = np.zeros_like(s_channel)\n",
    "    binary_output[(s_channel > thresh[0]) & (s_channel <= thresh[1])]=1\n",
    "    # 3) Return a binary image of threshold result\n",
    "    #binary_output = np.copy(img) # placeholder line\n",
    "    return binary_output\n",
    "\n",
    "def pipeline(img, s_thresh=(170, 255), sx_thresh=(20, 100)):\n",
    "    img = np.copy(img)\n",
    "    # Convert to HSV color space and separate the V channel\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HLS).astype(np.float)\n",
    "    l_channel = hsv[:,:,1]\n",
    "    s_channel = hsv[:,:,2]\n",
    "    # Sobel x\n",
    "    sobelx = cv2.Sobel(l_channel, cv2.CV_64F, 1, 0) # Take the derivative in x\n",
    "    abs_sobelx = np.absolute(sobelx) # Absolute x derivative to accentuate lines away from horizontal\n",
    "    scaled_sobel = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "    \n",
    "    # Threshold x gradient\n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= sx_thresh[0]) & (scaled_sobel <= sx_thresh[1])] = 1\n",
    "    \n",
    "    # Threshold color channel\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])] = 1\n",
    "    # Stack each channel\n",
    "    # Note color_binary[:, :, 0] is all 0s, effectively an all black image. It might\n",
    "    # be beneficial to replace this channel with something else.\n",
    "    color_binary = np.dstack(( np.zeros_like(sxbinary), sxbinary, s_binary))\n",
    "    return color_binary\n",
    "    \n",
    "result = pipeline(image)\n",
    "\n",
    "def window_mask(width, height, img_ref, center,level):\n",
    "    output = np.zeros_like(img_ref)\n",
    "    output[int(img_ref.shape[0]-(level+1)*height):int(img_ref.shape[0]-level*height),max(0,int(center-width/2)):min(int(center+width/2),img_ref.shape[1])] = 1\n",
    "    return output\n",
    "\n",
    "def find_window_centroids(image, window_width, window_height, margin):\n",
    "    \n",
    "    window_centroids = [] # Store the (left,right) window centroid positions per level\n",
    "    window = np.ones(window_width) # Create our window template that we will use for convolutions\n",
    "    \n",
    "    # First find the two starting positions for the left and right lane by using np.sum to get the vertical image slice\n",
    "    # and then np.convolve the vertical image slice with the window template \n",
    "    \n",
    "    # Sum quarter bottom of image to get slice, could use a different ratio\n",
    "    l_sum = np.sum(warped[int(3*warped.shape[0]/4):,:int(warped.shape[1]/2)], axis=0)\n",
    "    l_center = np.argmax(np.convolve(window,l_sum))-window_width/2\n",
    "    r_sum = np.sum(warped[int(3*warped.shape[0]/4):,int(warped.shape[1]/2):], axis=0)\n",
    "    r_center = np.argmax(np.convolve(window,r_sum))-window_width/2+int(warped.shape[1]/2)\n",
    "    \n",
    "    # Add what we found for the first layer\n",
    "    window_centroids.append((l_center,r_center))\n",
    "    \n",
    "    # Go through each layer looking for max pixel locations\n",
    "    for level in range(1,(int)(warped.shape[0]/window_height)):\n",
    "        # convolve the window into the vertical slice of the image\n",
    "        image_layer = np.sum(warped[int(warped.shape[0]-(level+1)*window_height):int(warped.shape[0]-level*window_height),:], axis=0)\n",
    "        conv_signal = np.convolve(window, image_layer)\n",
    "        # Find the best left centroid by using past left center as a reference\n",
    "        # Use window_width/2 as offset because convolution signal reference is at right side of window, not center of window\n",
    "        offset = window_width/2\n",
    "        l_min_index = int(max(l_center+offset-margin,0))\n",
    "        l_max_index = int(min(l_center+offset+margin,warped.shape[1]))\n",
    "        l_center = np.argmax(conv_signal[l_min_index:l_max_index])+l_min_index-offset\n",
    "        # Find the best right centroid by using past right center as a reference\n",
    "        r_min_index = int(max(r_center+offset-margin,0))\n",
    "        r_max_index = int(min(r_center+offset+margin,warped.shape[1]))\n",
    "        r_center = np.argmax(conv_signal[r_min_index:r_max_index])+r_min_index-offset\n",
    "        # Add what we found for that layer\n",
    "        window_centroids.append((l_center,r_center))\n",
    "\n",
    "    return window_centroids\n",
    "\n",
    "    window_centroids = find_window_centroids(warped, window_width, window_height, margin)\n",
    "\n",
    "    # If we found any window centers\n",
    "    if len(window_centroids) > 0:\n",
    "\n",
    "        # Points used to draw all the left and right windows\n",
    "        l_points = np.zeros_like(warped)\n",
    "        r_points = np.zeros_like(warped)\n",
    "\n",
    "        # Go through each level and draw the windows \t\n",
    "        for level in range(0,len(window_centroids)):\n",
    "            # Window_mask is a function to draw window areas\n",
    "            l_mask = window_mask(window_width,window_height,warped,window_centroids[level][0],level)\n",
    "            r_mask = window_mask(window_width,window_height,warped,window_centroids[level][1],level)\n",
    "            # Add graphic points from window mask here to total pixels found \n",
    "            l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\n",
    "            r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\n",
    "\n",
    "        # Draw the results\n",
    "        template = np.array(r_points+l_points,np.uint8) # add both left and right window pixels together\n",
    "        zero_channel = np.zeros_like(template) # create a zero color channel\n",
    "        template = np.array(cv2.merge((zero_channel,template,zero_channel)),np.uint8) # make window pixels green\n",
    "        warpage = np.array(cv2.merge((warped,warped,warped)),np.uint8) # making the original road pixels 3 color channels\n",
    "        output = cv2.addWeighted(warpage, 1, template, 0.5, 0.0) # overlay the orignal road image with window results\n",
    " \n",
    "    else:\n",
    "        output = np.array(cv2.merge((warped,warped,warped)),np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply a distortion correction to raw images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
